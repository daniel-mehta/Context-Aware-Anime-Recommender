{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891c2b4d-0752-4ff8-bb09-2a2b1b1a64dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3408d2-9e3e-4b21-bdfe-2d357c5be08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, amp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed025f7-c2de-44b9-917d-add996c7c7b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Rebuild Data Splits (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454dd98-3999-4150-883f-05ef51501673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned + mapped data\n",
    "anime = pd.read_csv(\"data/anime.csv\")\n",
    "ratings = pd.read_csv(\"data/rating.csv\")\n",
    "ratings = ratings[ratings.rating != -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2b7c4-1269-4346-a5b2-4eef319e53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep anime with sufficient ratings\n",
    "MIN_RATINGS_PER_ANIME = 10\n",
    "valid_anime_ids = ratings.groupby(\"anime_id\").size()\n",
    "valid_anime_ids = valid_anime_ids[valid_anime_ids >= MIN_RATINGS_PER_ANIME].index\n",
    "ratings = ratings[ratings.anime_id.isin(valid_anime_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5b009-23c0-47be-b8d4-da898d6e043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedback buckets\n",
    "def to_feedback(r):\n",
    "    if r <= 4: return 0\n",
    "    elif r <= 7: return 1\n",
    "    else: return 2\n",
    "ratings[\"feedback\"] = ratings[\"rating\"].apply(to_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0d5ae-1a1f-4781-b838-4e144f8187a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contiguous ids\n",
    "uid_map = {u:i for i,u in enumerate(ratings.user_id.unique())}\n",
    "iid_map = {a:j for j,a in enumerate(ratings.anime_id.unique())}\n",
    "ratings[\"uid\"] = ratings.user_id.map(uid_map)\n",
    "ratings[\"iid\"] = ratings.anime_id.map(iid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d1264-5602-41da-ad2e-170bfc4fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.sort_values([\"uid\"])\n",
    "ratings[\"timestamp\"] = ratings.groupby(\"uid\").cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b74131-3dbc-4d9d-88d4-8671c13c9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the same SeqDataset class here or import it\n",
    "MAX_SEQ_LEN = 50\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, max_len=50, split=\"train\"):\n",
    "        self.max_len = max_len\n",
    "        self.samples = []\n",
    "        df = df.sort_values([\"uid\",\"timestamp\"])\n",
    "        for uid, g in df.groupby(\"uid\"):\n",
    "            items = g[\"iid\"].to_numpy()\n",
    "            fbs   = g[\"feedback\"].to_numpy()\n",
    "            if len(items) < 3:\n",
    "                continue\n",
    "            if split == \"train\":\n",
    "                seq_items, seq_fbs, target = items[:-2], fbs[:-2], items[-2]\n",
    "            elif split == \"val\":\n",
    "                seq_items, seq_fbs, target = items[:-1], fbs[:-1], items[-1]\n",
    "            else:  # test for reporting\n",
    "                seq_items, seq_fbs, target = items[:-1], fbs[:-1], items[-1]\n",
    "            self.samples.append((seq_items, seq_fbs, target))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        items, fbs, target = self.samples[idx]\n",
    "        items = items[-self.max_len:]\n",
    "        fbs   = fbs[-self.max_len:]\n",
    "        L = len(items)\n",
    "        iid_seq = np.zeros(self.max_len, dtype=np.int64)\n",
    "        fb_seq  = np.zeros(self.max_len, dtype=np.int64)  # 0 pad\n",
    "        attn    = np.zeros(self.max_len, dtype=np.int64)\n",
    "        iid_seq[-L:] = items\n",
    "        fb_seq[-L:]  = fbs + 1  # 1..3 for dislike/like/love, 0 is pad\n",
    "        attn[-L:]    = 1\n",
    "        return (torch.tensor(iid_seq),\n",
    "                torch.tensor(fb_seq),\n",
    "                torch.tensor(attn),\n",
    "                torch.tensor(target, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ead79b-b62d-470d-b799-493932bce5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds  = SeqDataset(ratings, max_len=MAX_SEQ_LEN, split=\"val\")\n",
    "test_ds = SeqDataset(ratings, max_len=MAX_SEQ_LEN, split=\"test\")\n",
    "val_dl  = DataLoader(val_ds, batch_size=512, shuffle=False, num_workers=0)\n",
    "test_dl = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "num_items = len(iid_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8064a217-3ec6-4627-babe-1039650928b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509daf8-5a73-4add-9d89-5502f6f677a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(logits, targets, k=20):\n",
    "    topk = torch.topk(logits, k, dim=1).indices\n",
    "    hits = (topk == targets.view(-1,1)).any(dim=1).float()\n",
    "    return hits.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2cc9e-63f4-425e-a68f-f406195fad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(logits, targets, k=20):\n",
    "    topk = torch.topk(logits, k, dim=1).indices\n",
    "    # DCG is 1/log2(rank+2) if target in topk\n",
    "    gains = torch.zeros(logits.size(0), dtype=torch.float32, device=logits.device)\n",
    "    for i in range(topk.size(0)):\n",
    "        match = torch.nonzero(topk[i] == targets[i], as_tuple=False)\n",
    "        if match.numel() > 0:\n",
    "            rank = match[0,0].item()\n",
    "            gains[i] = 1.0 / torch.log2(torch.tensor(rank + 2.0, device=logits.device))\n",
    "    return gains.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c97fbe-d7bc-48fd-b6d2-7cab9eff51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, device, mask_feedback=False, genre_blend=None):\n",
    "    model.eval()\n",
    "    rec10 = rec20 = ndcg10 = ndcg20 = 0.0\n",
    "    n = 0\n",
    "    for iid_seq, fb_seq, attn, target in dataloader:\n",
    "        iid_seq = iid_seq.to(device)\n",
    "        fb_seq  = fb_seq.to(device)\n",
    "        attn    = attn.to(device)\n",
    "        target  = target.to(device)\n",
    "        if mask_feedback:\n",
    "            fb_seq = torch.zeros_like(fb_seq)  # remove fb signal\n",
    "\n",
    "        logits = model(iid_seq, fb_seq, attn)\n",
    "\n",
    "        if genre_blend is not None:\n",
    "            # blend on CPU-friendly chunk (optional)\n",
    "            logits = genre_blend(iid_seq, fb_seq, attn, logits)\n",
    "\n",
    "        rec10 += recall_at_k(logits, target, k=10) * iid_seq.size(0)\n",
    "        rec20 += recall_at_k(logits, target, k=20) * iid_seq.size(0)\n",
    "        ndcg10 += ndcg_at_k(logits, target, k=10) * iid_seq.size(0)\n",
    "        ndcg20 += ndcg_at_k(logits, target, k=20) * iid_seq.size(0)\n",
    "        n += iid_seq.size(0)\n",
    "    return {\n",
    "        \"Recall@10\": rec10 / n,\n",
    "        \"Recall@20\": rec20 / n,\n",
    "        \"NDCG@10\": ndcg10 / n,\n",
    "        \"NDCG@20\": ndcg20 / n\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd2839-6bdd-4ea2-9c5f-991ec055690a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Popularity Baseline (PopRec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b7816-e861-43c2-b51e-96859111ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity by rating count\n",
    "pop_counts = ratings.groupby(\"iid\").size().sort_values(ascending=False)\n",
    "pop_rank = torch.tensor(pop_counts.index.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f1bc2-5eed-4529-8297-31047fe235e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_pop(dataloader):\n",
    "    rec10 = rec20 = ndcg10 = ndcg20 = 0.0\n",
    "    n = 0\n",
    "    for _, _, _, target in dataloader:\n",
    "        # make fake logits by assigning descending scores by global popularity\n",
    "        # easiest way: rank vector where higher score for more popular\n",
    "        logits = torch.full((target.size(0), num_items), -1e9)\n",
    "        # assign a descending score to top-N popular items\n",
    "        topN = pop_rank[:500]  # 500 popular candidates\n",
    "        scores = torch.linspace(0, 1, steps=topN.numel())\n",
    "        logits[:, topN] = scores\n",
    "        rec10 += recall_at_k(logits, target, k=10) * target.size(0)\n",
    "        rec20 += recall_at_k(logits, target, k=20) * target.size(0)\n",
    "        ndcg10 += ndcg_at_k(logits, target, k=10) * target.size(0)\n",
    "        ndcg20 += ndcg_at_k(logits, target, k=20) * target.size(0)\n",
    "        n += target.size(0)\n",
    "    return {\n",
    "        \"Recall@10\": rec10 / n,\n",
    "        \"Recall@20\": rec20 / n,\n",
    "        \"NDCG@10\": ndcg10 / n,\n",
    "        \"NDCG@20\": ndcg20 / n\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e791c-116d-4ad3-b5d6-2e99863cf861",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_metrics = evaluate_pop(val_dl)\n",
    "pop_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cba0a-253c-4d47-949a-a1a98567849a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Load Trained SASRec and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29989e8b-07e4-4115-b61e-b08a352c3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12347d4-67df-4a4e-89c3-a84d21a073ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRec(nn.Module):\n",
    "    def __init__(self, num_items, fb_vocab=4, max_len=50, d_model=128, n_heads=2, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embeddings\n",
    "        self.item_emb = nn.Embedding(num_items, d_model, padding_idx=0)\n",
    "        self.fb_emb   = nn.Embedding(fb_vocab, d_model, padding_idx=0)  # 0=pad, 1=dislike, 2=like, 3=love\n",
    "        self.pos_emb  = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "\n",
    "        # Output: tie weights to item_emb\n",
    "        self.out_bias = nn.Parameter(torch.zeros(num_items))\n",
    "\n",
    "    def forward(self, iid_seq, fb_seq, attn_mask):\n",
    "        \"\"\"\n",
    "        iid_seq: [B, T]\n",
    "        fb_seq:  [B, T]\n",
    "        attn_mask: [B, T]\n",
    "        \"\"\"\n",
    "        B, T = iid_seq.size()\n",
    "        pos_ids = torch.arange(T, device=iid_seq.device).unsqueeze(0).expand(B, T)\n",
    "\n",
    "        # sum embeddings\n",
    "        x = self.item_emb(iid_seq) + self.fb_emb(fb_seq) + self.pos_emb(pos_ids)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Causal mask (prevent peeking ahead)\n",
    "        causal_mask = torch.triu(torch.ones(T, T, device=iid_seq.device), diagonal=1).bool()\n",
    "\n",
    "        # Padding mask (True where padded)\n",
    "        key_padding_mask = attn_mask == 0\n",
    "\n",
    "        h = self.encoder(x, mask=causal_mask, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        # Take the representation of the last valid position\n",
    "        last_idx = attn_mask.sum(dim=1) - 1\n",
    "        last_idx = last_idx.clamp(min=0)\n",
    "        h_last = h[torch.arange(B, device=iid_seq.device), last_idx]\n",
    "\n",
    "        # Score against all items\n",
    "        logits = h_last @ self.item_emb.weight.T + self.out_bias\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149ab81-f432-4135-b595-20202022f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SASRec(\n",
    "    num_items=num_items,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_layers=3,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(\"checkpoints/sasrec_final.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd699d-cbd0-4913-b2d6-2f39d01c94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sasrec = evaluate_model(model, val_dl, device)\n",
    "metrics_sasrec_maskfb = evaluate_model(model, val_dl, device, mask_feedback=True)\n",
    "\n",
    "metrics_sasrec, metrics_sasrec_maskfb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9663c3-fe18-4fb3-9ce9-c7928963ae9a",
   "metadata": {},
   "source": [
    "### Notes on Ablation Results\n",
    "The masked feedback variant shows a consistent drop across Recall and NDCG compared to the full SASRec with feedback.  \n",
    "This ablation was done by zeroing out the feedback embeddings at inference time (no retraining).  \n",
    "Therefore, the gap here is likely **understated**, a model trained entirely without feedback input would probably perform even worse.  \n",
    "Still, the drop is enough to confirm that incorporating feedback signals (dislike/like/love) adds meaningful predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f83236-05d4-4555-85bf-b6cfdc717fe3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Genre-Aware Blend at Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76d434-b53e-43fa-aa96-c9432bf28af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build genre matrix once\n",
    "all_genres = sorted({g.strip() for row in anime[\"genre\"].dropna() for g in row.split(\",\")})\n",
    "g2i = {g:i for i,g in enumerate(all_genres)}\n",
    "G = np.zeros((num_items, len(all_genres)), dtype=np.float32)\n",
    "for _, row in anime.iterrows():\n",
    "    a = row[\"anime_id\"]\n",
    "    if a in iid_map and isinstance(row[\"genre\"], str):\n",
    "        i = iid_map[a]\n",
    "        for g in row[\"genre\"].split(\",\"):\n",
    "            g = g.strip()\n",
    "            if g in g2i:\n",
    "                G[i, g2i[g]] = 1.0\n",
    "G = G / (np.linalg.norm(G, axis=1, keepdims=True) + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a5974-9113-46d3-8360-1822637c7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_user_content_vec(iid_seq_tensor, fb_seq_tensor, K=10):\n",
    "    # reverse the rightmost non-pad window\n",
    "    items = iid_seq_tensor.cpu().numpy()\n",
    "    fbs   = fb_seq_tensor.cpu().numpy() - 1  # back to 0..2\n",
    "    idx = items[items != 0][-K:]\n",
    "    fbv = fbs[fbs >= 0][-K:]\n",
    "    if idx.size == 0:\n",
    "        return np.zeros(G.shape[1], dtype=np.float32)\n",
    "    w = {0:1.0, 1:2.0, 2:3.0}  # dislike, like, love\n",
    "    u = np.zeros(G.shape[1], dtype=np.float32)\n",
    "    total = 0.0\n",
    "    for i, f in zip(idx, fbv):\n",
    "        u += w[int(f)] * G[int(i)]\n",
    "        total += w[int(f)]\n",
    "    if total > 0:\n",
    "        u /= total\n",
    "    u = u / (np.linalg.norm(u) + 1e-9)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc717150-0646-46ab-98ae-87233347b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_genre_blend(alpha=0.2):\n",
    "    def blend(iid_seq, fb_seq, attn, logits):\n",
    "        # per batch blend\n",
    "        B = logits.size(0)\n",
    "        add = torch.zeros_like(logits)\n",
    "        for b in range(B):\n",
    "            uvec = build_user_content_vec(iid_seq[b], fb_seq[b])\n",
    "            cs = torch.from_numpy(G @ uvec).to(logits.device)\n",
    "            add[b] = cs\n",
    "        return (1 - alpha) * logits + alpha * add\n",
    "    return blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb54f5-df06-4836-86b4-b5f5c7000bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sasrec_genre = evaluate_model(model, val_dl, device, mask_feedback=False, genre_blend=make_genre_blend(alpha=0.2))\n",
    "metrics_sasrec_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef00c91-73f5-4c8a-9f84-50a880939383",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Results Table and Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41149f0b-d5bf-4f6e-88b9-73f7fc2d143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    (\"Popularity\",            evaluate_pop(val_dl)),\n",
    "    (\"SASRec (feedback)\",     metrics_sasrec),\n",
    "    (\"SASRec (masked fb)\",    metrics_sasrec_maskfb),\n",
    "    (\"SASRec + genre blend\",  metrics_sasrec_genre),\n",
    "]\n",
    "df = pd.DataFrame([{ \"Model\": name, **m } for name, m in rows])\n",
    "display(df)\n",
    "\n",
    "ax = df.set_index(\"Model\")[\"Recall@20\"].plot(kind=\"bar\", rot=15)\n",
    "ax.set_title(\"Recall@20 by model\")\n",
    "ax.set_ylabel(\"Recall@20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb5790-472a-4016-baf3-e3a0cb2c6e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
